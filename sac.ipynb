{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor Critic Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from policy import CategoricalPolicy, GaussianPolicy\n",
    "from network_utils import build_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "\n",
    "        self.value = build_mlp(obs_dim, 1, 2, 64)\n",
    "        self.value_optimizer = optim.AdamW(self.value.parameters())\n",
    "\n",
    "        self.q = build_mlp(obs_dim + act_dim, 1, 2, 64)\n",
    "        self.q_optimizer = optim.AdamW(self.q.parameters())\n",
    "\n",
    "        self.network = build_mlp(obs_dim, act_dim, 2, 64)\n",
    "        self.policy = GaussianPolicy(self.network, self.act_dim)\n",
    "        self.policy_optimizer = optim.AdamW(self.policy.parameters())\n",
    "\n",
    "        # In the form of (s_t, a_t, r(s_t, a-t), s_{t+1})\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        self.action_sample_n = 100\n",
    "\n",
    "    # Accpting input as torch\n",
    "    def forward(self, obs):\n",
    "        dist = self.policy.action_distribution(obs)\n",
    "        return torch.tanh(dist.sample())\n",
    "\n",
    "    # Accept numpy\n",
    "    # def act(self, obs):\n",
    "        \n",
    "\n",
    "    def add_to_replay_buffer(self, obs, action, reward, next_obs, done):\n",
    "        self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "\n",
    "    def update_value_approx(self):\n",
    "        self.value_optimizer.zero_grad()\n",
    "\n",
    "        # TODO: we can probably have this be better by doing something better\n",
    "        # for s_t ~ D\n",
    "        loss = 0\n",
    "        for s in self.replay_buffer:\n",
    "            dist = self.policy.action_distribution(self, torch.array([[s]]))\n",
    "            t1 = self.value(s[0])\n",
    "            t2 = 0\n",
    "            for i in range(self.action_sample_n):\n",
    "                a = dist.sample()\n",
    "                t2 += self.q(torch.cat((s[0], a), 1)) - dist.log_prob(a)\n",
    "            t2 /= self.action_sample_n\n",
    "\n",
    "            loss += torch.square(t1 + t2)\n",
    "        loss /= 2 * len(self.replay_buffer)\n",
    "\n",
    "        loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "    def update_value(self):\n",
    "        self.value_optimizer.zero_grad()\n",
    "\n",
    "        loss = 0\n",
    "        for r in self.replay_buffer:\n",
    "            s = r[0]\n",
    "            dist = self.policy.action_distribution(self, torch.array([[s]]))\n",
    "            a = dist.sample() \n",
    "            loss += self.value(s) * (self.value(s) - self.q(torch.cat((s, a), 1)) + dist.log_sample(a))\n",
    "\n",
    "        loss /= len(self.replay_buffer)\n",
    "\n",
    "        loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "    def update_q(self):\n",
    "        self.q_optimizer.zero_grad()\n",
    "\n",
    "        loss = 0\n",
    "        for r in self.replay_buffer:\n",
    "            s = r[0]\n",
    "            dist = self.policy.action_distribution(self, torch.array([[s]]))\n",
    "            a = dist.sample() \n",
    "            loss += self.value(s) * (self.value(s) - self.q(torch.cat((s, a), 1)) + dist.log_sample(a))\n",
    "\n",
    "        loss /= len(self.replay_buffer)\n",
    "\n",
    "        loss.backward() \n",
    "        self.q_optimizer.step()\n",
    "\n",
    "    # def update_q(self):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collect:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SoftActorCritic.add_to_replay_buffer() takes 2 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 41\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# print(f\"Episode {episode+1}: Reward = {episode_reward}\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# Now do a bunch of gradient steps\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Run the training loop\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_episodes, max_steps)\u001b[0m\n\u001b[1;32m     22\u001b[0m action \u001b[38;5;241m=\u001b[39m agent(obs_)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     23\u001b[0m next_obs, reward, done, info, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 25\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_to_replay_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m obs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[1;32m     28\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mTypeError\u001b[0m: SoftActorCritic.add_to_replay_buffer() takes 2 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "env = gym.make('Ant-v4')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "# Define your Soft Actor-Critic agent\n",
    "agent = SoftActorCritic(obs_dim, act_dim)\n",
    "\n",
    "gradient_steps = 100\n",
    "\n",
    "# Training loop\n",
    "def train(num_episodes, max_steps):\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        obs = env.reset()[0]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        # Collect trajectory data into replay buffer\n",
    "        # TODO: should replay buffer be a set or a list?\n",
    "        for step in range(max_steps):\n",
    "            obs_ = torch.tensor(obs, dtype=torch.float32)\n",
    "            action = agent(obs_).detach().numpy()\n",
    "            next_obs, reward, done, info, _ = env.step(action)\n",
    "            \n",
    "            agent.add_to_replay_buffer(obs, action, reward, next_obs, done)\n",
    "\n",
    "            obs = next_obs\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # print(f\"Episode {episode+1}: Reward = {episode_reward}\")\n",
    "\n",
    "        # Now do a bunch of gradient steps\n",
    "        for step in range(gradient_steps):\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "# Run the training loop\n",
    "train(num_episodes=100, max_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs234_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
